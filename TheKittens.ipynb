{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TheKittens.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TandonArchit/Heart-Watch/blob/main/TheKittens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heart-Watch**\n",
        "\n",
        "> This is a prototype and work in progress.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ko3CMJgtNWBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make a prediction, we need to train a machine learning model using a large dataset. In our case, we use a set of 70,000 people, with and without a heart condition. "
      ],
      "metadata": {
        "id": "KGauo5r1NzrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start off by uploading our CSV file with the data here."
      ],
      "metadata": {
        "id": "E3PFpxQoOQ73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "CrFC5Q1zlv58",
        "outputId": "06438677-0cb8-452d-aa6c-22a294145e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9b9d2e84-a0c4-4e2f-9928-8b9d4a61ccc0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9b9d2e84-a0c4-4e2f-9928-8b9d4a61ccc0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cardio_train.csv to cardio_train (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the following 'cleanData' function is used to open the CSV file, clean its data and return the data in a usable format"
      ],
      "metadata": {
        "id": "3R9q0xAmNLUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanData():\n",
        "  \n",
        "  # First, we import the required libraries.\n",
        "  import pandas\n",
        "\n",
        "  # We start by using the pandas to read the data from our csv file.\n",
        "  testData = pandas.read_csv('cardio_train.csv', sep = ';')\n",
        "\n",
        "  # Now, we need to clean the data by removing unwanted columns like 'id'.\n",
        "  testData = testData.drop('id', axis = 1) \n",
        "\n",
        "  # Return our clean data\n",
        "  return testData"
      ],
      "metadata": {
        "id": "XFX_LJXls4HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'dataSplit' function splits the data into two datasets to efficiently form the model and to test its output to measure accuracy"
      ],
      "metadata": {
        "id": "4Mgz1oG5OsuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataSplit(fullData):\n",
        "\n",
        "  # First, we import the required libraries.\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Now, we can use sklearn to split our dataset into train and test.\n",
        "  train1, test1, train2, test2 = train_test_split(fullData.iloc[:, :-1].values, fullData.iloc[:, -1].values, test_size = 0.20, random_state = 13)\n",
        "\n",
        "  # We now have our test and train datasets, so we now we will return them as a tuple.\n",
        "  return (train1, train2, test1, test2)"
      ],
      "metadata": {
        "id": "bSUDNw7g03a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'modelLearn' function takes in the final, transformed datasets and makes a machine learning model using RandomForestClassifier"
      ],
      "metadata": {
        "id": "7_3sItFhM3DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modelLearn(datasets):\n",
        "\n",
        "  # First, we import the required libraries.\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  # Now, we will train the model.\n",
        "  model = RandomForestClassifier(criterion = 'entropy', random_state = 13)\n",
        "  model.fit(datasets[0], datasets[1])\n",
        "\n",
        "  # Our model is now ready, so we can return it.\n",
        "  return model"
      ],
      "metadata": {
        "id": "jhtGrmZP044k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'testValue' function is meant to use the machine learning model that we built to perform a new prediction, on new data"
      ],
      "metadata": {
        "id": "hRk2-7iNMh-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testValue(model, newValue):\n",
        "\n",
        "  tester = model\n",
        "  testResult = tester.predict([newValue])\n",
        "\n",
        "  if testResult[0] == 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "OOS4vIcK08U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function brings all the above defined functions together and serves the purpose of making our ML model user friendly and usable"
      ],
      "metadata": {
        "id": "xI_13Z56MCWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def runFunc():\n",
        "  # Here we first import the required libraries\n",
        "  import time\n",
        "  from IPython.display import clear_output\n",
        "\n",
        "  clear_output()\n",
        "  print(\"Initializing...\")\n",
        "\n",
        "  # Now we can start running the functions one by one, to display user friendly errors if they occur\n",
        "\n",
        "  # cleanData function\n",
        "  try:\n",
        "    cleandata = cleanData()\n",
        "    print(\"CSV File successfully read\")\n",
        "  except:\n",
        "    print(\"There was an error while reading the CSV file.\")\n",
        "    return\n",
        "\n",
        "  # dataSplit function\n",
        "  try:\n",
        "    datatuple = dataSplit(cleandata)\n",
        "  except:\n",
        "    print(\"There was an error while handling the data.\")\n",
        "    return\n",
        "\n",
        "  # modelLearn function\n",
        "  try:\n",
        "    model = modelLearn(datatuple)\n",
        "    print(\"ML Model was successfully created.\")\n",
        "  except:\n",
        "    print(\"There was an error while making the ML Model.\")\n",
        "    return\n",
        "\n",
        "  print(\"Initialization Complete.\")\n",
        "  time.sleep(3)\n",
        "  clear_output()\n",
        "\n",
        "  # Here, we start a loop to keep accepting inputs until the user decides to stop the function\n",
        "  while True:\n",
        "    print(\"{Type 'help' or 'h' for help.}\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    # We use the input function to accept user input\n",
        "    userinput = input(\">> \")\n",
        "\n",
        "    # Now we can go through the following and match the userinput with the function that it is supposed to perform\n",
        "\n",
        "    # 'q' or 'quit'\n",
        "    if userinput.lower() == 'q' or userinput.lower() == 'quit':\n",
        "      clear_output()\n",
        "      print(\"Function Stopped.\")\n",
        "      break\n",
        "\n",
        "    # 'h' or 'help'\n",
        "    elif userinput.lower() == 'h'or userinput.lower() == 'help' or 'help' in userinput:\n",
        "      print(\"\"\"\n",
        "      'r' or 'run' --> Run the model on the set array\n",
        "      'cls' or 'clear' --> Clear the screen\n",
        "      'testr' or 'test trainset' --> Calculate accuracy on the dataset the model was trained on\n",
        "      'testn' or 'test newset' --> Calculate accuracy on the dataset that is completely new to the model\n",
        "      'remo' or 'remodel' --> Make a fresh ML model\n",
        "      'q' or 'quit' --> Stop the function\n",
        "      'h' or 'help' --> See this list again\n",
        "\n",
        "      \"\"\")\n",
        "\n",
        "    # 'r' or 'run'\n",
        "    elif userinput.lower() == 'r' or userinput.lower() == 'run':\n",
        "      try:\n",
        "        testname = newValue()\n",
        "        if testValue(model, testname):\n",
        "          print(\"No Cardiac issue detected \\n\")\n",
        "        else:\n",
        "          print(\"Cardiac issue detected \\n\")\n",
        "      except Exception as inst:\n",
        "        print(\"\\n Please make sure the array name you entered was correct. \\n\", inst)\n",
        "\n",
        "    # 'cls' or 'clear'\n",
        "    elif userinput.lower() == 'cls' or userinput.lower() == 'clear':\n",
        "      clear_output()\n",
        "\n",
        "    # 'testt' or 'test trainset'\n",
        "    elif userinput.lower() == 'testt' or userinput.lower() == 'test trainset':\n",
        "      print(\"Running Tests...\")\n",
        "      testModel = model\n",
        "      score = testModel.score(datatuple[0], datatuple[1])\n",
        "      print(\"Current Accuracy: \", round(score * 100, 3), \"\\n\")\n",
        "\n",
        "    # 'testn' or 'test newset'\n",
        "    elif userinput.lower() == 'testn' or userinput.lower() == 'test newset':\n",
        "      print(\"Running Tests...\")\n",
        "      testModel = model\n",
        "      score = testModel.score(datatuple[2], datatuple[3])\n",
        "      print(\"Current Accuracy: \", round(score * 100, 3), \"\\n\")\n",
        "\n",
        "    # 'remo' or 'remodel'\n",
        "    elif userinput.lower() == 'remo' or userinput.lower() == 'remodel':\n",
        "      try:\n",
        "        model = modelLearn(datatuple)\n",
        "        print(\"ML Model was successfully created. \\n\")\n",
        "      except:\n",
        "        print(\"There was an error while making the ML Model.\")\n",
        "\n",
        "    # Invalid Inputs, that is, all other inputs\n",
        "    else:\n",
        "      print(\"Function not recognized. Type 'h' or 'help' for help. \\n\")\n",
        "\n",
        "    ## END\n"
      ],
      "metadata": {
        "id": "3JP2MHBU1FKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can try out values to see what our model would classify them as! We have some values set as default but you can play with them as you like. When you type in the 'run' command on the final function, the values in the cell below are given to the model to predict an output on. Be sure to run the following cell after making edits :) "
      ],
      "metadata": {
        "id": "gpIaeujlLbm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newValue():\n",
        "\n",
        "  age = 22113\n",
        "\n",
        "  gender = 1\n",
        "\n",
        "  height = 157\n",
        "\n",
        "  weight = 93.0\n",
        "\n",
        "  high = 130\n",
        "\n",
        "  low = 80\n",
        "\n",
        "  cholesterol = 3\n",
        "\n",
        "  glucose = 1\n",
        "\n",
        "  smoke = 0\n",
        "  \n",
        "  alcohol = 0\n",
        "\n",
        "  active = 1\n",
        "\n",
        "  return [age, gender, height, weight, high, low, cholesterol, glucose, smoke, alcohol, active]"
      ],
      "metadata": {
        "id": "9D3uX4zR_Ii_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Function:**"
      ],
      "metadata": {
        "id": "TgYoihPTL1M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runFunc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRw3XGr3F4sr",
        "outputId": "121ff3b7-07d3-4647-8c5e-6cbbceeed44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function Stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backup values"
      ],
      "metadata": {
        "id": "OtFEP-SZbfCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def newValue():\n",
        "\n",
        "#   age = 22113\n",
        "\n",
        "#   gender = 1\n",
        "\n",
        "#   height = 157\n",
        "\n",
        "#   weight = 93.0\n",
        "\n",
        "#   high = 130\n",
        "\n",
        "#   low = 80\n",
        "\n",
        "#   cholesterol = 3\n",
        "\n",
        "#   glucose = 1\n",
        "\n",
        "#   smoke = 0\n",
        "  \n",
        "#   alcohol = 0\n",
        "\n",
        "#   active = 1\n",
        "\n",
        "#   return [age, gender, height, weight, high, low, cholesterol, glucose, smoke, alcohol, active]"
      ],
      "metadata": {
        "id": "iS6qyTaLZ3C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how the fitbit API call would work and give us the user data in JSON format"
      ],
      "metadata": {
        "id": "scs82vv1bMVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fitbitConnect():\n",
        "  import requests\n",
        "  tokenID = ''\n",
        "  head = {'Authorization': 'Bearer {}'.format(tokenID)}\n",
        "  callData = requests.get(\"https://api.fitbit.com/1/user/-/profile.json\", headers = head).json()\n",
        "  return callData"
      ],
      "metadata": {
        "id": "_MQ3BSO_ajvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the end of our prototype."
      ],
      "metadata": {
        "id": "l5le9USgbql0"
      }
    }
  ]
}